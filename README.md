# Deep Reinforcement Learning — A2C on CartPole

**Notebook:** `RL_A2C_FINAL.ipynb`
**Env:** OpenAI Gym `CartPole-v1`
**Algo:** Advantage Actor–Critic (A2C) with a shared MLP (PyTorch)

**Authors**

* Sina Beyrami — 400105433
* Aren GolAzizian — 99171366
* M. Hossein HajiHosseini — 99101427

---

## Project Overview

We train an agent to balance the classic **CartPole** using an **A2C** policy/value network:

* **Actor** outputs a categorical policy over left/right actions.
* **Critic** estimates the state value.
* **Loss =** policy (with advantage) **+** value MSE.
* **Returns** are computed with discounted rewards (no GAE).
* Videos are rendered to MP4 for both a **random policy** and the **trained agent**.

The trained model consistently achieves **500/500** reward in evaluation episodes.

---

## Repository Structure

```
Deep-RL-A2C-on-CartPole/
├── RL_A2C_FINAL.ipynb        # End-to-end notebook (train + eval + video)
├── cartpole.mp4              # Random policy (generated by the notebook)
├── test.mp4                  # Trained agent rollout (generated by the notebook)
└── README.md                 # This file
```

> The MP4 files are created when you run the notebook cells.

---

## Requirements

* Python 3.9+
* PyTorch
* NumPy, tqdm
* Gym (classic) **or** Gymnasium (see compatibility note below)
* imageio, imageio-ffmpeg (for video)
* IPython (for inline video in notebooks)
* (Optional) OpenCV; Matplotlib

Quick install (Colab-like):

```bash
pip install "gym[atari,accept-rom-license]" imageio imageio-ffmpeg tqdm torch numpy
```

### Gym API compatibility

* The notebook uses the older `env.step(...) -> obs, reward, done, info` signature and `env.render(mode='rgb_array')`.
* With **Gym ≤ 0.25**, this works as-is.
* With **Gym ≥ 0.26 / Gymnasium**, prefer:

  * `env = gym.make('CartPole-v1', render_mode='rgb_array')`
  * `obs, reward, terminated, truncated, info = env.step(action)`
  * `done = terminated or truncated`

If you see a deprecation warning about `render(mode=...)`, use `render_mode` during env creation.

---

## How to Run

1. Open `RL_A2C_FINAL.ipynb` in Jupyter/Colab.
2. Run the cells top-to-bottom:

   * Installs & imports
   * Random agent demo → saves `cartpole.mp4`
   * A2C implementation & training (defaults below)
   * Evaluation → saves `test.mp4` and prints average reward

> If `imageio` warns about macro block size resizing, it’s safe to ignore (or set `macro_block_size=1`).

---

## Model & Training Details

**Network (shared torso):**

* MLP: `Linear(input, 256) → ReLU → Linear(256, 256) → ReLU`
* **Actor head:** `Linear(256, n_actions)` + `softmax`
* **Critic head:** `Linear(256, 1)`

**Hyperparameters (defaults in notebook):**

* Episodes: `5000`
* Max steps/episode: `500`
* Discount γ: `0.99`
* Optimizer: `Adam(lr=1e-3)`
* Hidden size: `256`

**Losses:**

* Actor: `-log π(a|s) * (R − V(s))`
* Critic: `MSE( V(s), R )`
* Total: `actor_loss + critic_loss`

**Returns:** simple discounted sum (no bootstrapping/GAE).

---

## Results

* Training converges to **max score (500)** on `CartPole-v1`.
* Evaluation over 10 episodes typically shows **Average Reward ≈ 500**.
* A video of a trained rollout is written to **`test.mp4`**.

---

## Rendering & Videos

* **Random policy video:** `cartpole.mp4`
* **Trained policy video:** `test.mp4`

If videos don’t play:

* Ensure `imageio-ffmpeg` is installed.
* Some players require width/height divisible by 16; the script auto-resizes with a warning.

---

## Reproducibility Tips

The notebook does not fix seeds by default. For repeatable runs, set:

```python
import torch, numpy as np, random
seed = 42
torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)
env.reset(seed=seed)
```

With Gymnasium: `env.action_space.seed(seed); env.observation_space.seed(seed)`

---

## Extensions / Ideas

* Entropy bonus to encourage exploration.
* Advantage normalization and reward normalization.
* GAE(λ) for lower-variance advantages.
* Gradient clipping for stability.
* Learning-rate schedulers or early stopping.
* Multiple environments (A2C with vectorized envs).
* Port to Gymnasium and modern API fully.

---

## Acknowledgments

* OpenAI Gym CartPole environment
* PyTorch

---

## License

MIT © Sina Beyrami

---

### Troubleshooting

* Deprecation about `render(mode=...)`: create the env with `render_mode='rgb_array'`.
* Type errors in `env.step`: you’re likely on Gym ≥ 0.26; handle `terminated/truncated` separately.
* Slow training: reduce `num_episodes`, use GPU if available (`torch.cuda.is_available()`), or smaller hidden size.

---
